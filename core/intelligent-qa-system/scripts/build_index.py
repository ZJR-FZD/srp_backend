"""
æ„å»ºå‘é‡ç´¢å¼•è„šæœ¬
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import settings
from src.document_loader.pdf_loader import PDFLoader
from src.document_loader.docx_loader import DOCXLoader
from src.document_loader.markdown_loader import MarkdownLoader
from src.text_processor.splitter import SemanticSplitter, MarkdownStructuredSplitter
from src.text_processor.cleaner import TextCleaner
from src.vector_store.store_manager import VectorStoreManager


def load_all_documents():
    """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    print("\n" + "="*60)
    print("ğŸ“š åŠ è½½æ–‡æ¡£")
    print("="*60)
    
    all_documents = []
    
    # åŠ è½½ PDF
    pdf_loader = PDFLoader()
    pdf_dir = settings.PDF_DIR
    if any(pdf_dir.glob("*.pdf")):
        print(f"\nğŸ“„ åŠ è½½ PDF æ–‡ä»¶...")
        pdf_docs = pdf_loader.load_directory(str(pdf_dir))
        all_documents.extend(pdf_docs)
        print(f"   âœ… PDF: {len(pdf_docs)} ä¸ªé¡µé¢")
    
    # åŠ è½½ Word
    docx_loader = DOCXLoader()
    docx_dir = settings.DOCX_DIR
    if any(docx_dir.glob("*.docx")) or any(docx_dir.glob("*.doc")):
        print(f"\nğŸ“„ åŠ è½½ Word æ–‡ä»¶...")
        docx_docs = docx_loader.load_directory(str(docx_dir))
        all_documents.extend(docx_docs)
        print(f"   âœ… Word: {len(docx_docs)} ä¸ªæ–‡æ¡£")
    
    # åŠ è½½ Markdown
    md_loader = MarkdownLoader()
    md_dir = settings.MARKDOWN_DIR
    if any(md_dir.glob("*.md")) or any(md_dir.glob("*.markdown")):
        print(f"\nğŸ“„ åŠ è½½ Markdown æ–‡ä»¶...")
        md_docs = md_loader.load_directory(str(md_dir))
        all_documents.extend(md_docs)
        print(f"   âœ… Markdown: {len(md_docs)} ä¸ªæ–‡æ¡£")
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(all_documents)} ä¸ªæ–‡æ¡£")
    print(f"{'='*60}")
    
    return all_documents


def process_documents(documents):
    """å¤„ç†æ–‡æ¡£ï¼šæ¸…æ´—å’Œåˆ‡åˆ†"""
    print("\n" + "="*60)
    print("ğŸ”§ å¤„ç†æ–‡æ¡£")
    print("="*60)
    
    # æ–‡æœ¬æ¸…æ´—
    print(f"\nğŸ§¹ æ¸…æ´—æ–‡æœ¬...")
    cleaner = TextCleaner(
        remove_urls=True,
        remove_emails=True,
        remove_extra_whitespace=True,
        remove_special_chars=False,
        lowercase=False
    )
    cleaned_docs = cleaner.clean_documents(documents)
    print(f"   âœ… æ¸…æ´—å®Œæˆ: {len(cleaned_docs)} ä¸ªæ–‡æ¡£")
    
    # ğŸ‘‡ æ–°å¢ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆ†å—å™¨
    print(f"\nâœ‚ï¸  åˆ‡åˆ†æ–‡æœ¬...")
    
    # æ£€æµ‹æ˜¯å¦ä¸º Markdown æ–‡æ¡£
    md_docs = [doc for doc in cleaned_docs if doc.metadata.get("filename", "").endswith(".md")]
    other_docs = [doc for doc in cleaned_docs if doc not in md_docs]
    
    all_chunks = []
    
    # Markdown æ–‡æ¡£ä½¿ç”¨ç»“æ„åŒ–åˆ†å—å™¨
    if md_docs:
        print(f"   ğŸ“ ä½¿ç”¨ Markdown ç»“æ„åŒ–åˆ†å—å™¨å¤„ç† {len(md_docs)} ä¸ª .md æ–‡ä»¶")
        md_splitter = MarkdownStructuredSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            keep_heading_hierarchy=True,  # ä¿ç•™æ ‡é¢˜å±‚çº§
            split_list_items=True,         # åˆ—è¡¨é¡¹ç‹¬ç«‹åˆ†å—
            max_heading_levels=3           # æœ€å¤šä¿ç•™3çº§æ ‡é¢˜
        )
        md_chunks = md_splitter.split_documents(md_docs)
        all_chunks.extend(md_chunks)
        print(f"   âœ… Markdown åˆ‡åˆ†å®Œæˆ: {len(md_chunks)} ä¸ªå—")
    
    # å…¶ä»–æ–‡æ¡£ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨
    if other_docs:
        print(f"   ğŸ“„ ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨å¤„ç† {len(other_docs)} ä¸ªå…¶ä»–æ–‡ä»¶")
        semantic_splitter = SemanticSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP
        )
        other_chunks = semantic_splitter.split_documents(other_docs)
        all_chunks.extend(other_chunks)
        print(f"   âœ… è¯­ä¹‰åˆ‡åˆ†å®Œæˆ: {len(other_chunks)} ä¸ªå—")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“„ ç¤ºä¾‹å—ï¼ˆå‰3ä¸ªï¼‰:")
    for i, chunk in enumerate(all_chunks[:3], 1):
        print(f"\n   === å— {i} ===")
        print(f"   æ ‡é¢˜è·¯å¾„: {chunk.metadata.get('heading_path', [])}")
        print(f"   ç±»å‹: {chunk.metadata.get('section_type', 'N/A')}")
        print(f"   å†…å®¹é¢„è§ˆ:")
        print(f"   {chunk.content[:150]}{'...' if len(chunk.content) > 150 else ''}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    avg_length = sum(len(chunk.content) for chunk in all_chunks) / len(all_chunks) if all_chunks else 0
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - åŸå§‹æ–‡æ¡£: {len(documents)}")
    print(f"   - æ¸…æ´—å: {len(cleaned_docs)}")
    print(f"   - åˆ‡åˆ†å: {len(all_chunks)}")
    print(f"   - å¹³å‡å—å¤§å°: {avg_length:.0f} å­—ç¬¦")
    
    return all_chunks

def build_vector_index(documents, embedding_model=None):
    """æ„å»ºå‘é‡ç´¢å¼•"""
    # åˆ›å»ºå‘é‡å­˜å‚¨ç®¡ç†å™¨
    manager = VectorStoreManager(embedding_model=embedding_model)
    
    # æ„å»ºç´¢å¼•
    store = manager.build_index(
        documents=documents,
        batch_size=32,
        save=True
    )
    
    return manager

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸš€ "*30)
    print("å‘é‡ç´¢å¼•æ„å»ºå·¥å…·")
    print("ğŸš€ "*30)
    
    # æ˜¾ç¤ºé…ç½®
    settings.display()
    
    # 1. åŠ è½½æ–‡æ¡£
    documents = load_all_documents()
    
    if not documents:
        print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼")
        print("   è¯·åœ¨ä»¥ä¸‹ç›®å½•æ·»åŠ æ–‡æ¡£:")
        print(f"   - PDF: {settings.PDF_DIR}")
        print(f"   - Word: {settings.DOCX_DIR}")
        print(f"   - Markdown: {settings.MARKDOWN_DIR}")
        return
    
    # 2. å¤„ç†æ–‡æ¡£
    chunks = process_documents(documents)
    
    # 3. æ„å»ºç´¢å¼•
    manager = build_vector_index(
        chunks,
        embedding_model=settings.EMBEDDING_MODEL
    )
    
    # 4. æ˜¾ç¤ºç»Ÿè®¡
    stats = manager.get_stats()
    print("\n" + "="*60)
    print("ğŸ“Š ç´¢å¼•ç»Ÿè®¡")
    print("="*60)
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    print("\n" + "="*60)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ’¾ ç´¢å¼•å·²ä¿å­˜åˆ°: {settings.VECTOR_STORE_DIR}")

if __name__ == "__main__":
    main()