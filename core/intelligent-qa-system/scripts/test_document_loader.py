"""
æµ‹è¯•æ–‡æ¡£åŠ è½½å’Œå¤„ç†æµç¨‹
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import settings
from src.document_loader.pdf_loader import PDFLoader
from src.document_loader.docx_loader import DOCXLoader
from src.document_loader.markdown_loader import MarkdownLoader
from src.text_processor.splitter import TextSplitter, SemanticSplitter
from src.text_processor.cleaner import TextCleaner


def test_pdf_loader():
    """æµ‹è¯• PDF åŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯• PDF åŠ è½½")
    print("="*60)
    
    loader = PDFLoader()
    pdf_dir = settings.PDF_DIR
    
    if not any(pdf_dir.glob("*.pdf")):
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶ï¼Œè¯·åœ¨ data/documents/pdfs/ ç›®å½•ä¸‹æ”¾ç½® PDF æ–‡ä»¶")
        return
    
    documents = loader.load_directory(str(pdf_dir))
    
    print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    if documents:
        print(f"\nç¤ºä¾‹æ–‡æ¡£:")
        doc = documents[0]
        print(f"  æ¥æº: {doc.metadata['source']}")
        print(f"  é¡µç : {doc.metadata.get('page_number', 'N/A')}")
        print(f"  å†…å®¹é¢„è§ˆ: {doc.content[:200]}...")


def test_docx_loader():
    """æµ‹è¯• Word åŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯• Word æ–‡æ¡£åŠ è½½")
    print("="*60)
    
    loader = DOCXLoader()
    docx_dir = settings.DOCX_DIR
    
    if not any(docx_dir.glob("*.docx")) and not any(docx_dir.glob("*.doc")):
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ° Word æ–‡ä»¶ï¼Œè¯·åœ¨ data/documents/docx/ ç›®å½•ä¸‹æ”¾ç½® Word æ–‡ä»¶")
        return
    
    documents = loader.load_directory(str(docx_dir))
    
    print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    if documents:
        print(f"\nç¤ºä¾‹æ–‡æ¡£:")
        doc = documents[0]
        print(f"  æ¥æº: {doc.metadata['source']}")
        print(f"  æ®µè½æ•°: {doc.metadata.get('paragraphs_count', 'N/A')}")
        print(f"  å†…å®¹é¢„è§ˆ: {doc.content[:200]}...")


def test_markdown_loader():
    """æµ‹è¯• Markdown åŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯• Markdown æ–‡æ¡£åŠ è½½")
    print("="*60)
    
    loader = MarkdownLoader()
    md_dir = settings.MARKDOWN_DIR
    
    if not any(md_dir.glob("*.md")) and not any(md_dir.glob("*.markdown")):
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ° Markdown æ–‡ä»¶ï¼Œè¯·åœ¨ data/documents/markdown/ ç›®å½•ä¸‹æ”¾ç½® Markdown æ–‡ä»¶")
        return
    
    documents = loader.load_directory(str(md_dir))
    
    print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    if documents:
        print(f"\nç¤ºä¾‹æ–‡æ¡£:")
        doc = documents[0]
        print(f"  æ¥æº: {doc.metadata['source']}")
        print(f"  æ ‡é¢˜: {doc.metadata.get('title', 'N/A')}")
        print(f"  å†…å®¹é¢„è§ˆ: {doc.content[:200]}...")


def test_text_splitter():
    """æµ‹è¯•æ–‡æœ¬åˆ‡åˆ†"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ–‡æœ¬åˆ‡åˆ†")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬
    test_text = """
    è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡å­—ã€‚è¿™æ®µæ–‡å­—åŒ…å«äº†ä¸€äº›å†…å®¹ï¼Œç”¨æ¥æµ‹è¯•æ–‡æœ¬åˆ‡åˆ†åŠŸèƒ½ã€‚
    æˆ‘ä»¬éœ€è¦ç¡®ä¿åˆ‡åˆ†åçš„æ–‡æœ¬å—å¤§å°åˆé€‚ï¼Œå¹¶ä¸”ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
    
    è¿™æ˜¯ç¬¬äºŒæ®µæ–‡å­—ã€‚å®ƒç»§ç»­è®¨è®ºæ–‡æœ¬åˆ‡åˆ†çš„é‡è¦æ€§ã€‚
    å¥½çš„æ–‡æœ¬åˆ‡åˆ†å¯ä»¥æé«˜æ£€ç´¢è´¨é‡å’Œç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚
    æ¯ä¸ªæ–‡æœ¬å—åº”è¯¥åŒ…å«å®Œæ•´çš„è¯­ä¹‰å•å…ƒã€‚
    
    è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡å­—ã€‚å®ƒæ€»ç»“äº†å‰é¢çš„å†…å®¹ã€‚
    """
    
    from src.document_loader.base_loader import Document
    
    doc = Document(
        content=test_text,
        metadata={"source": "test", "filename": "test.txt"}
    )
    
    # æµ‹è¯•æ ‡å‡†åˆ‡åˆ†
    splitter = TextSplitter(chunk_size=100, chunk_overlap=20)
    chunks = splitter.split_documents([doc])
    
    print(f"\næ ‡å‡†åˆ‡åˆ†:")
    print(f"  åŸå§‹æ–‡æ¡£: 1 ä¸ª")
    print(f"  åˆ‡åˆ†å: {len(chunks)} ä¸ªæ–‡æœ¬å—")
    print(f"\nå‰3ä¸ªæ–‡æœ¬å—:")
    for i, chunk in enumerate(chunks[:3]):
        print(f"\n  å— {i+1} (é•¿åº¦: {len(chunk.content)}):")
        print(f"  {chunk.content[:80]}...")
    
    # æµ‹è¯•è¯­ä¹‰åˆ‡åˆ†
    semantic_splitter = SemanticSplitter(chunk_size=150, chunk_overlap=30)
    semantic_chunks = semantic_splitter.split_documents([doc])
    
    print(f"\n\nè¯­ä¹‰åˆ‡åˆ†:")
    print(f"  åˆ‡åˆ†å: {len(semantic_chunks)} ä¸ªæ–‡æœ¬å—")


def test_text_cleaner():
    """æµ‹è¯•æ–‡æœ¬æ¸…æ´—"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ–‡æœ¬æ¸…æ´—")
    print("="*60)
    
    test_text = """
    è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬  ï¼ŒåŒ…å«å¤šä½™çš„   ç©ºæ ¼ã€‚
    
    
    è¿˜æœ‰å¤šä½™çš„æ¢è¡Œã€‚
    
    åŒ…å« URL: https://example.com å’Œé‚®ç®±: test@example.com
    """
    
    from src.document_loader.base_loader import Document
    
    doc = Document(
        content=test_text,
        metadata={"source": "test"}
    )
    
    cleaner = TextCleaner(
        remove_urls=True,
        remove_emails=True,
        remove_extra_whitespace=True
    )
    
    cleaned_docs = cleaner.clean_documents([doc])
    
    print(f"\nåŸå§‹æ–‡æœ¬:")
    print(repr(test_text))
    print(f"\næ¸…æ´—å:")
    print(repr(cleaned_docs[0].content))


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "ğŸš€ "*30)
    print("æ–‡æ¡£åŠ è½½ä¸å¤„ç†æµ‹è¯•")
    print("ğŸš€ "*30)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    settings.create_directories()
    
    # æ˜¾ç¤ºé…ç½®
    settings.display()
    
    # è¿è¡Œæµ‹è¯•
    test_pdf_loader()
    test_docx_loader()
    test_markdown_loader()
    test_text_splitter()
    test_text_cleaner()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  1. åœ¨ data/documents/ ç›®å½•ä¸‹æ·»åŠ ä½ çš„æ–‡æ¡£")
    print("  2. è¿è¡Œ build_index.py æ„å»ºå‘é‡ç´¢å¼•")
    print("  3. å¼€å§‹ä½¿ç”¨é—®ç­”ç³»ç»Ÿ\n")


if __name__ == "__main__":
    main()