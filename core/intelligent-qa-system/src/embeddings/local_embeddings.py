"""
æœ¬åœ° Embedding æ¨¡å‹
ä½¿ç”¨ sentence-transformers æœ¬åœ°æ¨¡å‹
"""
from typing import List
import numpy as np
from sentence_transformers import SentenceTransformer

from config.settings import settings

class LocalEmbeddings:
    """æœ¬åœ° Embedding æ¨¡å‹"""
    
    def __init__(self, model_name: str = None):
        """
        åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        """
        self.model_name = model_name or settings.LOCAL_EMBEDDING_MODEL
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹: {self.model_name}")
        
        try:
            # åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
            self.model = SentenceTransformer(self.model_name)
            self.dimension = self.model.get_sentence_embedding_dimension()
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‘é‡ç»´åº¦: {self.dimension}")
        except Exception as e:
            raise Exception(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            np.ndarray: æ–‡æœ¬å‘é‡
        """
        if not text or not text.strip():
            # è¿”å›é›¶å‘é‡
            return np.zeros(self.dimension, dtype=np.float32)
        
        try:
            embedding = self.model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=True  # L2 å½’ä¸€åŒ–
            )
            return embedding.astype(np.float32)
        except Exception as e:
            print(f"âš ï¸  æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
            return np.zeros(self.dimension, dtype=np.float32)
    
    def embed_texts(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> np.ndarray:
        """
        æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            np.ndarray: æ–‡æœ¬å‘é‡çŸ©é˜µ (n_texts, dimension)
        """
        if not texts:
            return np.array([]).reshape(0, self.dimension)
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text if text and text.strip() else " " for text in texts]
        
        try:
            embeddings = self.model.encode(
                valid_texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            return embeddings.astype(np.float32)
        except Exception as e:
            print(f"âš ï¸  æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡çŸ©é˜µ
            return np.zeros((len(texts), self.dimension), dtype=np.float32)
    
    def embed_query(self, query: str) -> np.ndarray:
        """
        å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼ˆä¸ embed_text ç›¸åŒï¼Œä½†è¯­ä¹‰æ›´æ¸…æ™°ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            np.ndarray: æŸ¥è¯¢å‘é‡
        """
        return self.embed_text(query)
    
    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension
    
    def similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2
            
        Returns:
            float: ä½™å¼¦ç›¸ä¼¼åº¦ (0-1)
        """
        # å› ä¸ºå·²ç»åšäº† L2 å½’ä¸€åŒ–ï¼Œç›´æ¥ç‚¹ç§¯å³å¯
        return float(np.dot(vec1, vec2))