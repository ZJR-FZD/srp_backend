"""
FAISS å‘é‡å­˜å‚¨
"""
import os
import json
from typing import List, Tuple, Dict, Any
from pathlib import Path
import numpy as np
import faiss
from tqdm import tqdm

from config.settings import settings
from ..document_loader.base_loader import Document


class FAISSStore:
    """FAISS å‘é‡å­˜å‚¨"""
    
    def __init__(self, dimension: int, index_type: str = None):
        """
        åˆå§‹åŒ– FAISS å­˜å‚¨
        
        Args:
            dimension: å‘é‡ç»´åº¦
            index_type: ç´¢å¼•ç±»å‹ ('Flat', 'IVFFlat', 'HNSW')
        """
        self.dimension = dimension
        self.index_type = index_type or settings.FAISS_INDEX_TYPE
        
        # åˆ›å»ºç´¢å¼•
        self.index = self._create_index()
        
        # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        self.documents: List[Document] = []
        self.id_to_index: Dict[int, int] = {}  # æ–‡æ¡£IDåˆ°ç´¢å¼•çš„æ˜ å°„
        
        print(f"âœ… FAISS ç´¢å¼•åˆå§‹åŒ–æˆåŠŸï¼ç±»å‹: {self.index_type}, ç»´åº¦: {dimension}")
    
    def _create_index(self) -> faiss.Index:
        """åˆ›å»º FAISS ç´¢å¼•"""
        if self.index_type == "Flat":
            # æš´åŠ›æœç´¢ï¼Œæœ€ç²¾ç¡®ä½†é€Ÿåº¦æ…¢
            index = faiss.IndexFlatL2(self.dimension)
        
        elif self.index_type == "IVFFlat":
            # å€’æ’æ–‡ä»¶ç´¢å¼•ï¼Œé€‚åˆå¤§æ•°æ®é›†
            # nlist: èšç±»ä¸­å¿ƒæ•°é‡ï¼Œå»ºè®®ä¸º sqrt(n_vectors)
            nlist = 100
            quantizer = faiss.IndexFlatL2(self.dimension)
            index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
            # æ³¨æ„ï¼šIVF éœ€è¦å…ˆè®­ç»ƒæ‰èƒ½æ·»åŠ å‘é‡
            self.needs_training = True
        
        elif self.index_type == "HNSW":
            # åˆ†å±‚å¯¼èˆªå°ä¸–ç•Œå›¾ï¼Œé€Ÿåº¦å¿«ä½†å ç”¨å†…å­˜å¤š
            M = 32  # æ¯å±‚çš„é‚»å±…æ•°
            index = faiss.IndexHNSWFlat(self.dimension, M)
            index.hnsw.efConstruction = 40  # æ„å»ºæ—¶çš„æœç´¢æ·±åº¦
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç´¢å¼•ç±»å‹: {self.index_type}")
        
        return index
    
    def add_documents(self, documents: List[Document], embeddings: np.ndarray):
        """
        æ·»åŠ æ–‡æ¡£å’Œå‘é‡
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            embeddings: å¯¹åº”çš„å‘é‡çŸ©é˜µ (n_docs, dimension)
        """
        if len(documents) != len(embeddings):
            raise ValueError("æ–‡æ¡£æ•°é‡å’Œå‘é‡æ•°é‡ä¸åŒ¹é…")
        
        # å¯¹å‘é‡åš L2 å½’ä¸€åŒ–
        faiss.normalize_L2(embeddings)

        # å¦‚æœæ˜¯ IVF ç´¢å¼•ä¸”æœªè®­ç»ƒï¼Œå…ˆè®­ç»ƒ
        if self.index_type == "IVFFlat" and not self.index.is_trained:
            print("ğŸ”„ æ­£åœ¨è®­ç»ƒ IVF ç´¢å¼•...")
            self.index.train(embeddings)
            print("âœ… ç´¢å¼•è®­ç»ƒå®Œæˆ")
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        start_id = len(self.documents)
        self.index.add(embeddings)
        
        # ä¿å­˜æ–‡æ¡£å’Œæ˜ å°„
        for i, doc in enumerate(documents):
            doc_id = start_id + i
            self.documents.append(doc)
            self.id_to_index[doc_id] = len(self.documents) - 1
        
        print(f"âœ… å·²æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ï¼Œå½“å‰æ€»æ•°: {len(self.documents)}")
    
    def search(
        self,
        query_vector: np.ndarray,
        k: int = None,
        threshold: float = None
    ) -> List[Tuple[Document, float]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£
        
        æ³¨æ„ï¼š
        - æ–‡æ¡£å‘é‡å’ŒæŸ¥è¯¢å‘é‡éƒ½ä¼šåš L2 å½’ä¸€åŒ–
        - FAISS è¿”å›çš„æ˜¯ L2 è·ç¦»ï¼ˆèŒƒå›´ [0, 2]ï¼‰
        - è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦ï¼šcosine = 1 - (distance^2 / 2)
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            k: è¿”å›æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ settings.TOP_Kï¼‰
            threshold: ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå»ºè®® 0.3-0.7ï¼‰
            
        Returns:
            List[Tuple[Document, float]]: (æ–‡æ¡£, ç›¸ä¼¼åº¦) åˆ—è¡¨
        """
        if len(self.documents) == 0:
            return []
        
        k = k or settings.TOP_K
        threshold = threshold if threshold is not None else settings.SIMILARITY_THRESHOLD
        
        # ç¡®ä¿æŸ¥è¯¢å‘é‡æ˜¯ 2D
        if query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)
        
        # L2 å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        faiss.normalize_L2(query_vector)
        
        # FAISS æœç´¢
        distances, indices = self.index.search(query_vector, k)
        
        results = []
        for rank, (dist, idx) in enumerate(zip(distances[0], indices[0]), start=1):
            if idx == -1:  # æ— æ•ˆç»“æœ
                continue
            
            # ğŸ‘‡ ä¿®å¤ï¼šæ­£ç¡®çš„ä½™å¼¦ç›¸ä¼¼åº¦è½¬æ¢å…¬å¼
            # å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼šL2(a,b)^2 = 2 - 2*cos(a,b)
            # => cos(a,b) = 1 - L2(a,b)^2 / 2
            cosine_sim = 1.0 - float(dist * dist) / 2.0
            
            # ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            if cosine_sim < threshold:
                continue
            
            doc = self.documents[idx]
            results.append((doc, cosine_sim))
        
        return results
    
    def get_document_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        return len(self.documents)
    
    def save(self, save_dir: str = None):
        """
        ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
        
        Args:
            save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ç›®å½•
        """
        save_dir = Path(save_dir) if save_dir else settings.VECTOR_STORE_DIR
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ FAISS ç´¢å¼•
        index_path = save_dir / settings.FAISS_INDEX_FILE
        faiss.write_index(self.index, str(index_path))
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "dimension": self.dimension,
            "index_type": self.index_type,
            "document_count": len(self.documents),
            "documents": [
                {
                    "content": doc.content,
                    "metadata": doc.metadata
                }
                for doc in self.documents
            ]
        }
        
        metadata_path = save_dir / settings.FAISS_METADATA_FILE
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {save_dir}")
        print(f"   - ç´¢å¼•æ–‡ä»¶: {index_path}")
        print(f"   - å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
    
    @classmethod
    def load(cls, load_dir: str = None) -> 'FAISSStore':
        """
        åŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®
        
        Args:
            load_dir: åŠ è½½ç›®å½•
            
        Returns:
            FAISSStore: åŠ è½½çš„å­˜å‚¨å®ä¾‹
        """
        load_dir = Path(load_dir) if load_dir else settings.VECTOR_STORE_DIR
        
        index_path = load_dir / settings.FAISS_INDEX_FILE
        metadata_path = load_dir / settings.FAISS_METADATA_FILE
        
        if not index_path.exists() or not metadata_path.exists():
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {load_dir}")
        
        # åŠ è½½å…ƒæ•°æ®
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # åˆ›å»ºå®ä¾‹
        store = cls(
            dimension=metadata['dimension'],
            index_type=metadata['index_type']
        )
        
        # åŠ è½½ FAISS ç´¢å¼•
        store.index = faiss.read_index(str(index_path))
        
        # æ¢å¤æ–‡æ¡£
        store.documents = [
            Document(
                content=doc_data['content'],
                metadata=doc_data['metadata']
            )
            for doc_data in metadata['documents']
        ]
        
        # é‡å»º ID æ˜ å°„
        store.id_to_index = {i: i for i in range(len(store.documents))}
        
        print(f"âœ… ç´¢å¼•å·²åŠ è½½: {load_dir}")
        print(f"   - æ–‡æ¡£æ•°é‡: {len(store.documents)}")
        print(f"   - ç´¢å¼•ç±»å‹: {store.index_type}")
        
        return store
    
    def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        self.index.reset()
        self.documents.clear()
        self.id_to_index.clear()
        print("âœ… ç´¢å¼•å·²æ¸…ç©º")